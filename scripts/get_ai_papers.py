"""è·å–AI/ç®—æ³•è®ºæ–‡"""
import os
import re
import math
import traceback
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

from collections import defaultdict
from ai_categories_config import CATEGORY_DISPLAY_ORDER, CATEGORY_THRESHOLDS, CATEGORY_KEYWORDS
from llm_helper import ChatGLMHelper
from typing import Dict, List, Tuple, Optional
import traceback
import arxiv

# æŸ¥è¯¢å‚æ•°è®¾ç½®
QUERY_DAYS_AGO = 1          # æŸ¥è¯¢å‡ å¤©å‰çš„è®ºæ–‡ï¼Œ0=ä»Šå¤©ï¼Œ1=æ˜¨å¤©ï¼Œ2=å‰å¤©
MAX_RESULTS = 600           # æœ€å¤§è¿”å›è®ºæ–‡æ•°é‡
MAX_WORKERS = 2            # å¹¶è¡Œå¤„ç†çš„æœ€å¤§çº¿ç¨‹æ•°

# ArXiv ç±»åˆ«é…ç½®ï¼ˆæ ¸å¿ƒAIç±»åˆ«ï¼‰
ARXIV_CATEGORIES = [
    "cs.AI",    # Artificial Intelligence
    "cs.CL",    # Computation and Language (NLP)
    "cs.CV",    # Computer Vision
    "cs.LG",    # Machine Learning
    "stat.ML",  # Statistics - Machine Learning
    "cs.NE",    # Neural and Evolutionary Computing
]

# å¯¼å…¥NLTKåº“ç”¨äºæ–‡æœ¬é¢„å¤„ç†
try:
    import nltk
    from nltk.stem import PorterStemmer, WordNetLemmatizer
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords
    
    # åˆ›å»ºæ ‡å¿—æ–‡ä»¶è·¯å¾„
    nltk_flag_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), '.nltk_data_downloaded')
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»ä¸‹è½½è¿‡NLTKæ•°æ®
    if os.path.exists(nltk_flag_file):
        # å·²ç»ä¸‹è½½è¿‡ï¼Œç›´æ¥ä½¿ç”¨
        NLTK_AVAILABLE = True
    else:
        # æ£€æŸ¥å¿…è¦çš„NLTKæ•°æ®æ˜¯å¦å·²ä¸‹è½½
        needed_data = []
        for data_name in ['punkt', 'wordnet', 'stopwords']:
            try:
                path = f"{'tokenizers/' if data_name == 'punkt' else 'corpora/'}{data_name}"
                nltk.data.find(path)
                print(f"NLTKæ•°æ® '{data_name}' å·²å­˜åœ¨äº: {path}")
            except LookupError:
                needed_data.append(data_name)
                print(f"NLTKæ•°æ® '{data_name}' ä¸å­˜åœ¨ï¼Œéœ€è¦ä¸‹è½½")
        
        # åªä¸‹è½½ç¼ºå¤±çš„æ•°æ®
        if needed_data:
            print(f"æ­£åœ¨ä¸‹è½½ç¼ºå¤±çš„NLTKæ•°æ®æ–‡ä»¶: {', '.join(needed_data)}")
            for data_name in needed_data:
                print(f"å¼€å§‹ä¸‹è½½ '{data_name}'...")
                download_result = nltk.download(data_name, quiet=False)
                print(f"ä¸‹è½½ '{data_name}' ç»“æœ: {download_result}")
            print("NLTKæ•°æ®æ–‡ä»¶ä¸‹è½½å®Œæˆ")
        
        # ç‰¹åˆ«å¤„ç†punkt_tab
        try:
            nltk.data.find('tokenizers/punkt_tab')
            print("NLTKæ•°æ® 'punkt_tab' å·²å­˜åœ¨")
        except LookupError:
            print("å¼€å§‹ä¸‹è½½ 'punkt_tab'...")
            download_result = nltk.download('punkt', quiet=False)  # é‡æ–°ä¸‹è½½ punktå¯èƒ½ä¼šåŒ…å«punkt_tab
            print(f"ä¸‹è½½ 'punkt' ç»“æœ: {download_result}")
        
        # åˆ›å»ºæ ‡å¿—æ–‡ä»¶è¡¨ç¤ºæ•°æ®å·²ä¸‹è½½
        with open(nltk_flag_file, 'w') as f:
            f.write(f"NLTK data downloaded at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        NLTK_AVAILABLE = True
    
    NLTK_AVAILABLE = True
except ImportError:
    print("NLTKåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŸºæœ¬æ–‡æœ¬å¤„ç†")
    NLTK_AVAILABLE = False

def extract_github_link(paper):
    """ä»è®ºæ–‡ä¸­æå–ä»£ç é“¾æ¥ï¼ˆGitHubã€é¡¹ç›®ä¸»é¡µç­‰ï¼‰

    Args:
        paper: arXivè®ºæ–‡å¯¹è±¡

    Returns:
        str: ä»£ç é“¾æ¥æˆ–None
    """
    # GitHubé“¾æ¥æ¨¡å¼
    github_patterns = [
        # GitHubé“¾æ¥
        r'https?://github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        r'github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        r'https?://www\.github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        r'www\.github\.com/[a-zA-Z0-9-]+/[a-zA-Z0-9-_.]+',
        # é¡¹ç›®é¡µé¢
        r'https?://[a-zA-Z0-9-]+\.github\.io/[a-zA-Z0-9-_.]+',
        # é€šç”¨ä»£ç é“¾æ¥æ¨¡å¼
        r'code.*available.*?(?:https?://github\.com/[^\s<>"]+)',
        r'implementation.*?(?:https?://github\.com/[^\s<>"]+)',
        r'source.*code.*?(?:https?://github\.com/[^\s<>"]+)',
    ]

    # è¦æœç´¢çš„æ–‡æœ¬æ¥æº
    text_sources = []
    
    # 1. æ‘˜è¦
    if hasattr(paper, 'summary') and paper.summary:
        text_sources.append(paper.summary)
    
    # 2. è¯„è®º
    if hasattr(paper, 'comments') and paper.comments:
        text_sources.append(paper.comments)
    
    # 3. æœŸåˆŠå¼•ç”¨
    if hasattr(paper, 'journal_ref') and paper.journal_ref:
        text_sources.append(paper.journal_ref)
    
    # 4. é“¾æ¥åˆ—è¡¨
    if hasattr(paper, 'links'):
        for link in paper.links:
            if hasattr(link, 'href') and link.href:
                text_sources.append(link.href)
    
    # 5. DOI
    if hasattr(paper, 'doi') and paper.doi:
        text_sources.append(paper.doi)
    
    # ä»æ‰€æœ‰æ–‡æœ¬æ¥æºä¸­æŸ¥æ‰¾GitHubé“¾æ¥
    for text in text_sources:
        for pattern in github_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                url = match.group(0)
                if not url.startswith('http'):
                    url = 'https://' + url
                return url
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°GitHubé“¾æ¥ï¼Œå°è¯•ä»linksä¸­æå–é¡¹ç›®ä¸»é¡µ
    if hasattr(paper, 'links'):
        for link in paper.links:
            if hasattr(link, 'href') and link.href:
                href = link.href
                # æ£€æŸ¥æ˜¯å¦æ˜¯é¡¹ç›®ä¸»é¡µï¼ˆéarXivã€éPDFï¼‰
                if (href and 
                    'arxiv.org' not in href.lower() and 
                    'pdf' not in href.lower() and
                    ('http://' in href or 'https://' in href)):
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¸¸è§ä»£ç ä»“åº“å…³é”®è¯
                    code_keywords = ['code', 'github', 'gitlab', 'bitbucket', 'project', 'demo', 'page']
                    if any(keyword in href.lower() for keyword in code_keywords):
                        return href
    
    return None


def extract_arxiv_id(url):
    """ä»ArXiv URLä¸­æå–è®ºæ–‡ID

    Args:
        url: ArXivè®ºæ–‡URL

    Returns:
        str: è®ºæ–‡ID
    """
    # å¤„ç†ä¸åŒæ ¼å¼çš„ArXiv URL
    patterns = [
        r"arxiv\.org/abs/(\d+\.\d+)",
        r"arxiv\.org/pdf/(\d+\.\d+)",
    ]

    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)

    return None


def df_to_markdown_table(papers_by_category: dict, target_date) -> str:
    """ç”Ÿæˆè¡¨æ ¼å½¢å¼çš„Markdownå†…å®¹ï¼Œæ”¯æŒä¸¤çº§ç±»åˆ«æ ‡é¢˜"""
    markdown = ""
    
    # è¿‡æ»¤æ‰æ²¡æœ‰è®ºæ–‡çš„ç±»åˆ«
    active_categories = {k: v for k, v in papers_by_category.items() if v}
    
    if not active_categories:
        return "ä»Šå¤©æ²¡æœ‰ç›¸å…³è®ºæ–‡ã€‚"
    
    # è¡¨æ ¼åˆ—æ ‡é¢˜
    headers = ['çŠ¶æ€', 'è‹±æ–‡æ ‡é¢˜', 'ä¸­æ–‡æ ‡é¢˜', 'ä½œè€…', 'PDFé“¾æ¥', 'ä»£ç /è´¡çŒ®']
    
    # æŒ‰ç…§CATEGORY_DISPLAY_ORDERçš„é¡ºåºå¤„ç†ç±»åˆ«
    for category in CATEGORY_DISPLAY_ORDER:
        if category not in active_categories:
            continue
        # åªè¾“å‡ºä¸€æ¬¡ä¸»ç±»åˆ«æ ‡é¢˜
        markdown += f"\n## {category}\n\n"
        papers_by_subcategory = defaultdict(list)
        for paper in active_categories[category]:
            subcategory = paper.get('subcategory', '')
            papers_by_subcategory[subcategory].append(paper)
        if not papers_by_subcategory:
            continue
        for subcategory, papers in papers_by_subcategory.items():
            markdown += f"\n### {subcategory}\n\n"
            markdown += "|" + "|".join(headers) + "|\n"
            markdown += "|" + "|".join(["---"] * len(headers)) + "|\n"
            for paper in papers:
                if paper['is_updated']:
                    status = f"ğŸ“ æ›´æ–°"
                else:
                    status = f"ğŸ†• å‘å¸ƒ"
                def summarize_contribution(core_contribution):
                    if not core_contribution:
                        return []
                    if "|" in core_contribution:
                        items = [item.strip() for item in core_contribution.split("|")]
                    else:
                        items = [core_contribution.strip()]
                    blacklist = ["ä»£ç å¼€æº", "æä¾›æ•°æ®é›†", "ä»£ç å·²å¼€æº", "æ•°æ®é›†å·²å…¬å¼€"]
                    items = [i for i in items if all(b not in i for b in blacklist)]
                    items = items[:2]
                    items = [(i[:50] + ("..." if len(i) > 50 else "")) for i in items]
                    return items
                contrib_list = []
                if "æ ¸å¿ƒè´¡çŒ®" in paper:
                    contrib_list = summarize_contribution(paper["æ ¸å¿ƒè´¡çŒ®"])
                if paper['github_url'] != 'None':
                    code_and_contribution = f"[ä»£ç ]({paper['github_url']})"
                    if contrib_list:
                        code_and_contribution += "; " + "; ".join(contrib_list)
                elif contrib_list:
                    code_and_contribution = "; ".join(contrib_list)
                else:
                    code_and_contribution = 'æ— '
                values = [
                    status,
                    paper['title'],
                    paper.get('title_zh', ''),
                    paper['authors'],
                    f"[PDF]({paper['pdf_url']})",
                    code_and_contribution,
                ]
                values = [str(v).replace('\n', ' ').replace('|', '&#124;') for v in values]
                markdown += "|" + "|".join(values) + "|\n"
            markdown += "\n"
    return markdown


def df_to_markdown_detailed(papers_by_category: dict, target_date) -> str:
    """ç”Ÿæˆè¯¦ç»†æ ¼å¼çš„Markdownå†…å®¹ï¼Œæ”¯æŒä¸¤çº§ç±»åˆ«æ ‡é¢˜"""
    markdown = ""
    
    # è¿‡æ»¤æ‰æ²¡æœ‰è®ºæ–‡çš„ç±»åˆ«
    active_categories = {k: v for k, v in papers_by_category.items() if v}
    
    if not active_categories:
        return "ä»Šå¤©æ²¡æœ‰ç›¸å…³è®ºæ–‡ã€‚"
    
    # æŒ‰ç…§CATEGORY_DISPLAY_ORDERçš„é¡ºåºå¤„ç†ç±»åˆ«
    for category in CATEGORY_DISPLAY_ORDER:
        if category not in active_categories:
            continue
            
        # æ·»åŠ ä¸€çº§ç±»åˆ«æ ‡é¢˜
        markdown += f"\n## {category}\n\n"
        
        # æŒ‰å­ç±»åˆ«ç»„ç»‡è®ºæ–‡
        papers_by_subcategory = defaultdict(list)
        
        # å°†æ‰€æœ‰è®ºæ–‡åˆ†é…åˆ°å­ç±»åˆ«
        for paper in active_categories[category]:
            subcategory = paper.get('subcategory', '')
            papers_by_subcategory[subcategory].append(paper)
        
        # å¦‚æœå½“å‰ç±»åˆ«ä¸‹æ²¡æœ‰è®ºæ–‡ï¼Œè·³è¿‡
        if not papers_by_subcategory:
            continue
            
        # å¤„ç†æ¯ä¸ªå­ç±»åˆ«
        for subcategory, papers in papers_by_subcategory.items():
            # æ·»åŠ äºŒçº§ç±»åˆ«æ ‡é¢˜
            markdown += f"\n### {subcategory}\n\n"
            
            # æ·»åŠ è®ºæ–‡è¯¦ç»†ä¿¡æ¯
            for idx, paper in enumerate(papers, 1):
                # å¼•ç”¨ç¼–å·
                markdown += f'**index:** {idx}<br />\n'
                # æ—¥æœŸ
                markdown += f'**Date:** {target_date.strftime("%Y-%m-%d")}<br />\n'
                # è‹±æ–‡æ ‡é¢˜
                markdown += f'**Title:** {paper["title"]}<br />\n'
                # ä¸­æ–‡æ ‡é¢˜
                markdown += f'**Title_cn:** {paper.get("title_zh", "")}<br />\n'
                # ä½œè€…ï¼ˆå·²ç»æ˜¯æ ¼å¼åŒ–å¥½çš„å­—ç¬¦ä¸²ï¼‰
                markdown += f'**Authors:** {paper["authors"]}<br />\n'
                # PDFé“¾æ¥
                markdown += f'**PDF:** [PDF]({paper["pdf_url"]})<br />\n'

                # åˆå¹¶ä»£ç é“¾æ¥å’Œç²¾ç®€åçš„æ ¸å¿ƒè´¡çŒ®
                markdown += '**Code/Contribution:**\n'
                
                # ç²¾ç®€æ ¸å¿ƒè´¡çŒ®å†…å®¹
                def summarize_contribution(core_contribution):
                    if not core_contribution:
                        return []
                    # åˆ†å‰²ä¸ºå¤šæ¡
                    if "|" in core_contribution:
                        items = [item.strip() for item in core_contribution.split("|")] 
                    else:
                        items = [core_contribution.strip()]
                    # å»é™¤æ¨¡æ¿åŒ–å†…å®¹
                    blacklist = ["ä»£ç å¼€æº", "æä¾›æ•°æ®é›†", "ä»£ç å·²å¼€æº", "æ•°æ®é›†å·²å…¬å¼€"]
                    items = [i for i in items if all(b not in i for b in blacklist)]
                    # åªä¿ç•™å‰ä¸‰æ¡
                    items = items[:3]
                    return items
                
                # å¤„ç†æ ¸å¿ƒè´¡çŒ®
                contrib_list = []
                if "æ ¸å¿ƒé—®é¢˜" in paper:
                    markdown += f'é—®é¢˜ï¼š{paper["æ ¸å¿ƒé—®é¢˜"]}\n'
                
                if "æ ¸å¿ƒæ–¹æ³•" in paper:
                    markdown += f'æ–¹æ³•ï¼š{paper["æ ¸å¿ƒæ–¹æ³•"]}\n'
                
                if "æ ¸å¿ƒè´¡çŒ®" in paper:
                    contrib_list = summarize_contribution(paper["æ ¸å¿ƒè´¡çŒ®"])
                    if contrib_list:
                        markdown += f'{", ".join(contrib_list)}\n'
                
                # å¤„ç†ä»£ç é“¾æ¥
                if paper['github_url'] != 'None':
                    markdown += f'[ä»£ç ]({paper["github_url"]})\n'
                
                # æ·»åŠ ç©ºè¡Œ
                markdown += '\n'

    return markdown


def preprocess_text(text: str) -> str:
    """
    å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼ŒåŒ…æ‹¬å°å†™è½¬æ¢ã€åˆ†è¯ã€å»åœç”¨è¯ã€è¯å¹²æå–å’Œè¯å½¢è¿˜åŸ
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        
    Returns:
        str: é¢„å¤„ç†åçš„æ–‡æœ¬
    """
    # è½¬æ¢ä¸ºå°å†™
    text = text.lower()
    
    # åŸºæœ¬æ–‡æœ¬å¤„ç†ï¼šå…ˆå»é™¤ç‰¹æ®Šå­—ç¬¦
    basic_processed = re.sub(r'[^\w\s]', ' ', text)
    
    # å¦‚æœNLTKä¸å¯ç”¨ï¼Œç›´æ¥è¿”å›åŸºæœ¬å¤„ç†ç»“æœ
    if not NLTK_AVAILABLE:
        return basic_processed
    
    # å°è¯•ä½¿ç”¨NLTKè¿›è¡Œé«˜çº§å¤„ç†
    try:
        # åˆ†è¯ - å…ˆä½¿ç”¨åŸºæœ¬åˆ†è¯ä½œä¸ºå¤‡é€‰
        try:
            tokens = word_tokenize(text)
        except Exception:
            # å¦‚æœé«˜çº§åˆ†è¯å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬åˆ†è¯
            tokens = basic_processed.split()
        
        # å»é™¤åœç”¨è¯
        try:
            stop_words = set(stopwords.words('english'))
            tokens = [token for token in tokens if token not in stop_words and len(token) > 2]
        except Exception:
            # å¦‚æœåœç”¨è¯å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬åœç”¨è¯åˆ—è¡¨
            basic_stop_words = {'a', 'an', 'the', 'in', 'on', 'at', 'for', 'to', 'of', 'and', 'or', 'with', 'by'}
            tokens = [token for token in tokens if token not in basic_stop_words and len(token) > 2]
        
        # è¯å¹²æå–å’Œè¯å½¢è¿˜åŸ - å¯é€‰åŠŸèƒ½
        try:
            stemmer = PorterStemmer()
            stemmed_tokens = [stemmer.stem(token) for token in tokens]
            
            lemmatizer = WordNetLemmatizer()
            lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]
            
            # é‡æ–°ç»„åˆæˆæ–‡æœ¬
            return " ".join(lemmatized_tokens)
        except Exception:
            # å¦‚æœè¯å¹²æå–æˆ–è¯å½¢è¿˜åŸå¤±è´¥ï¼Œåªè¿”å›åˆ†è¯å’Œå»åœç”¨è¯çš„ç»“æœ
            return " ".join(tokens)
    
    except Exception as e:
        print(f"NLTKå¤„ç†æ–‡æœ¬æ—¶å‡ºé”™: {str(e)}")
        # å¦‚æœæ‰€æœ‰NLTKå¤„ç†éƒ½å¤±è´¥ï¼Œå›é€€åˆ°åŸºæœ¬å¤„ç†
        return basic_processed


def get_category_by_keywords(title: str, abstract: str, categories_config: Dict) -> List[Tuple[str, float, Optional[Tuple[str, float]], Optional[Dict]]]:
    """
    æ‰§è¡ŒåŸºäºå…³é”®è¯åŒ¹é…å’Œä¼˜å…ˆçº§è§„åˆ™çš„å±‚æ¬¡åŒ–è®ºæ–‡åˆ†ç±»ï¼Œå¸¦æœ‰å¢å¼ºçš„æ–‡æœ¬å¤„ç†å’Œç½®ä¿¡åº¦è¯„åˆ†ã€‚
    
    Args:
        title (str): è®ºæ–‡æ ‡é¢˜ï¼Œç”¨äºä¸»è¦ä¸Šä¸‹æ–‡åˆ†æ
        abstract (str): è®ºæ–‡æ‘˜è¦ï¼Œç”¨äºå…¨é¢å†…å®¹åˆ†æ
        categories_config (Dict): åŒ…å«ç±»åˆ«å®šä¹‰ã€å…³é”®è¯ã€æƒé‡å’Œä¼˜å…ˆçº§çš„é…ç½®å­—å…¸
    
    å®ç°ç»†èŠ‚:
        1. å¢å¼ºæ–‡æœ¬é¢„å¤„ç†:
           - å¤§å°å†™æ ‡å‡†åŒ–å’Œæ ‡å‡†åŒ–å¤„ç†
           - æ ‡é¢˜å’Œæ‘˜è¦çš„ç»„åˆåˆ†æï¼Œä½¿ç”¨å·®å¼‚åŒ–æƒé‡
           - é«˜çº§åˆ†è¯å’Œåœç”¨è¯è¿‡æ»¤
           - å¤šçº§è¯å¹²æå–å’Œè¯å½¢è¿˜åŸ
           - N-gramåˆ†æï¼Œæé«˜çŸ­è¯­åŒ¹é…å‡†ç¡®æ€§
        
        2. ä¼˜åŒ–è¯„åˆ†æœºåˆ¶:
           - ä¸»è¦å¾—åˆ†: åŠ æƒå…³é”®è¯åŒ¹é… (åŠ¨æ€åŸºç¡€æƒé‡)
           - æ ‡é¢˜åŠ æˆ: æ ‡é¢˜åŒ¹é…çš„é¢å¤–æƒé‡ (ä¼˜åŒ–åŠ æƒ)
           - ç²¾ç¡®åŒ¹é…åŠ æˆ: å®Œæ•´çŸ­è¯­åŒ¹é…çš„é¢å¤–æƒé‡
           - ä¼˜å…ˆçº§ä¹˜æ•°: ç±»åˆ«ç‰¹å®šé‡è¦æ€§ç¼©æ”¾
           - è´Ÿé¢å…³é”®è¯æƒ©ç½š: ä½¿ç”¨æ”¹è¿›çš„é€»è¾‘å‡½æ•°å¹³æ»‘æƒ©ç½š
           - ç±»åˆ«ç›¸å…³æ€§åˆ¤æ–­: è€ƒè™‘ç±»åˆ«é—´çš„ç›¸å…³æ€§
        
        3. æ™ºèƒ½åˆ†ç±»é€»è¾‘:
           - ä½¿ç”¨ç±»åˆ«è‡ªå®šä¹‰é˜ˆå€¼ä¸åŠ¨æ€é˜ˆå€¼è°ƒæ•´
           - å¢å¼ºçš„å­ç±»åˆ«åˆ†ç±»
           - ä¼˜å…ˆç±»åˆ«çš„å±‚æ¬¡åŒ–å¤„ç†
           - æ™ºèƒ½å›é€€æœºåˆ¶ï¼Œè€ƒè™‘ç±»åˆ«ç›¸å…³æ€§
           - ç½®ä¿¡åº¦è¯„åˆ†å’Œåˆ†ç±»è§£é‡Š
    
    Returns:
        List[Tuple[str, float, Optional[Tuple[str, float]], Optional[Dict]]]: æŒ‰ç½®ä¿¡åº¦é™åºæ’åºçš„ 
        (ç±»åˆ«, ç½®ä¿¡åº¦åˆ†æ•°, å­ç±»åˆ«ä¿¡æ¯, åˆ†ç±»è§£é‡Š) å…ƒç»„åˆ—è¡¨
    """
    # æ–‡æœ¬é¢„å¤„ç†
    title_lower = title.lower()
    abstract_lower = abstract.lower()
    
    # ä½¿ç”¨é«˜çº§æ–‡æœ¬é¢„å¤„ç†
    processed_title = preprocess_text(title)
    processed_abstract = preprocess_text(abstract)
    processed_combined = processed_title + " " + processed_abstract
    
    # ç§»é™¤å¸¸è§çš„åœç”¨è¯ï¼Œæé«˜åŒ¹é…è´¨é‡
    stop_words = {'a', 'an', 'the', 'in', 'on', 'at', 'for', 'to', 'of', 'and', 'or', 'with', 'by', 
                 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 
                 'does', 'did', 'but', 'if', 'then', 'else', 'when', 'up', 'down', 'this', 'that'}
    
    # åˆ†è¯å¹¶è¿‡æ»¤åœç”¨è¯
    title_words = set(w for w in title_lower.split() if w not in stop_words)
    abstract_words = set(w for w in abstract_lower.split() if w not in stop_words)
    
    # ç»„åˆæ–‡æœ¬ç”¨äºåŒ¹é…
    combined_text = title_lower + " " + abstract_lower
    
    # åˆå§‹åŒ–å¾—åˆ†ç´¯åŠ å™¨å’ŒåŒ¹é…è®°å½•
    scores = defaultdict(float)
    match_details = defaultdict(list)
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¾—åˆ†
    for category, config in categories_config.items():
        score = 0.0
        matches = []
        
        # 1. æ­£å‘å…³é”®è¯åŒ¹é…
        for keyword, weight in config["keywords"]:
            keyword_lower = keyword.lower()
            keyword_words = set(w for w in keyword_lower.split() if w not in stop_words)
            
            # å¯¹å…³é”®è¯ä¹Ÿè¿›è¡Œé¢„å¤„ç†
            processed_keyword = preprocess_text(keyword)
            
            # å®Œæ•´çŸ­è¯­ç²¾ç¡®åŒ¹é…ï¼ˆæœ€é«˜æƒé‡ï¼‰
            if keyword_lower in title_lower:
                match_score = weight * 0.25  # æ ‡é¢˜ä¸­çš„ç²¾ç¡®åŒ¹é…æƒé‡æœ€é«˜
                score += match_score
                matches.append(f"æ ‡é¢˜ç²¾ç¡®åŒ¹é… [{keyword}]: +{match_score:.2f}")
            elif keyword_lower in abstract_lower:
                match_score = weight * 0.15  # æ‘˜è¦ä¸­çš„ç²¾ç¡®åŒ¹é…æƒé‡æ¬¡ä¹‹
                score += match_score
                matches.append(f"æ‘˜è¦ç²¾ç¡®åŒ¹é… [{keyword}]: +{match_score:.2f}")
            
            # ä½¿ç”¨é¢„å¤„ç†åçš„æ–‡æœ¬è¿›è¡ŒåŒ¹é…ï¼ˆæé«˜å‡†ç¡®æ€§ï¼‰
            elif processed_keyword in processed_title:
                match_score = weight * 0.22  # é¢„å¤„ç†æ ‡é¢˜ä¸­çš„åŒ¹é…æƒé‡é«˜
                score += match_score
                matches.append(f"æ ‡é¢˜è¯­ä¹‰åŒ¹é… [{keyword}]: +{match_score:.2f}")
            elif processed_keyword in processed_abstract:
                match_score = weight * 0.14  # é¢„å¤„ç†æ‘˜è¦ä¸­çš„åŒ¹é…æƒé‡ä¸­ç­‰
                score += match_score
                matches.append(f"æ‘˜è¦è¯­ä¹‰åŒ¹é… [{keyword}]: +{match_score:.2f}")
            
            # æ ‡é¢˜ä¸­çš„å…³é”®è¯ç»„åˆåŒ¹é…ï¼ˆé«˜æƒé‡ï¼‰
            elif len(keyword_words) > 1 and keyword_words.issubset(title_words):
                match_score = weight * 0.18  # æ ‡é¢˜ä¸­çš„è¯ç»„åŒ¹é…æƒé‡é«˜
                score += match_score
                matches.append(f"æ ‡é¢˜è¯ç»„åŒ¹é… [{keyword}]: +{match_score:.2f}")
            
            # æ‘˜è¦ä¸­çš„å…³é”®è¯ç»„åˆåŒ¹é…ï¼ˆä¸­ç­‰æƒé‡ï¼‰
            elif len(keyword_words) > 1 and keyword_words.issubset(abstract_words):
                match_score = weight * 0.12  # æ‘˜è¦ä¸­çš„è¯ç»„åŒ¹é…æƒé‡ä¸­ç­‰
                score += match_score
                matches.append(f"æ‘˜è¦è¯ç»„åŒ¹é… [{keyword}]: +{match_score:.2f}")
            
            # å•è¯åŒ¹é…ï¼ˆä½æƒé‡ï¼‰
            else:
                # å°†å…³é”®è¯æ‹†åˆ†ä¸ºå•è¯è¿›è¡ŒåŒ¹é…
                word_matches = 0
                title_match_bonus = 0
                
                # åˆ†åˆ«å¤„ç†åŸå§‹æ–‡æœ¬å’Œé¢„å¤„ç†æ–‡æœ¬çš„åŒ¹é…
                for word in keyword_words:
                    if len(word) <= 3:  # å¿½ç•¥è¿‡çŸ­çš„è¯
                        continue
                        
                    if word in title_words:
                        word_matches += 1
                        title_match_bonus += 1  # æ ‡é¢˜åŒ¹é…é¢å¤–åŠ åˆ†
                    elif word in abstract_words:
                        word_matches += 0.6  # æ‘˜è¦åŒ¹é…çš„æƒé‡ä½äºæ ‡é¢˜
                
                # å¤„ç†é¢„å¤„ç†æ–‡æœ¬ä¸­çš„åŒ¹é…
                processed_keyword_words = processed_keyword.split()
                for word in processed_keyword_words:
                    if len(word) <= 3:
                        continue
                    if word in processed_title:
                        word_matches += 0.5
                        title_match_bonus += 0.3
                    elif word in processed_abstract:
                        word_matches += 0.3
                
                # å¦‚æœæœ‰å•è¯åŒ¹é…ï¼Œè®¡ç®—å¾—åˆ†
                if word_matches > 0:
                    # è®¡ç®—åŒ¹é…æ¯”ä¾‹
                    match_ratio = word_matches / len(keyword_words)
                    # åŸºç¡€å¾—åˆ†
                    base_score = weight * match_ratio * 0.08
                    # æ ‡é¢˜åŒ¹é…åŠ æˆ
                    bonus_score = title_match_bonus * 0.02
                    # æ€»å¾—åˆ†
                    match_score = base_score + bonus_score
                    score += match_score
                    matches.append(f"å•è¯åŒ¹é… [{keyword}]: +{match_score:.2f} (åŒ¹é…åº¦: {match_ratio:.1%})")
        
        # 2. è´Ÿå‘å…³é”®è¯æƒ©ç½š
        negative_score = 0.0
        for neg_keyword, neg_weight in config.get("negative_keywords", []):
            neg_keyword_lower = neg_keyword.lower()
            neg_keyword_words = set(w for w in neg_keyword_lower.split() if w not in stop_words)
            
            # å®Œæ•´çŸ­è¯­åŒ¹é…ï¼ˆä¸¥é‡æƒ©ç½šï¼‰
            if neg_keyword_lower in title_lower:
                negative_score += neg_weight * 1.0  # æ ‡é¢˜ä¸­çš„è´Ÿå‘å…³é”®è¯ä¸¥é‡æƒ©ç½š
                matches.append(f"è´Ÿå‘å…³é”®è¯ [{neg_keyword}] åœ¨æ ‡é¢˜ä¸­: -{neg_weight:.2f}")
            elif neg_keyword_lower in abstract_lower:
                negative_score += neg_weight * 0.7  # æ‘˜è¦ä¸­çš„è´Ÿå‘å…³é”®è¯ä¸­åº¦æƒ©ç½š
                matches.append(f"è´Ÿå‘å…³é”®è¯ [{neg_keyword}] åœ¨æ‘˜è¦ä¸­: -{neg_weight * 0.7:.2f}")
            
            # å…³é”®è¯ç»„åˆåŒ¹é…ï¼ˆä¸­åº¦æƒ©ç½šï¼‰
            elif len(neg_keyword_words) > 1 and neg_keyword_words.issubset(title_words):
                negative_score += neg_weight * 0.8
                matches.append(f"è´Ÿå‘è¯ç»„ [{neg_keyword}] åœ¨æ ‡é¢˜ä¸­: -{neg_weight * 0.8:.2f}")
            elif len(neg_keyword_words) > 1 and neg_keyword_words.issubset(abstract_words):
                negative_score += neg_weight * 0.5
                matches.append(f"è´Ÿå‘è¯ç»„ [{neg_keyword}] åœ¨æ‘˜è¦ä¸­: -{neg_weight * 0.5:.2f}")
        
        # åº”ç”¨è´Ÿå‘å…³é”®è¯æƒ©ç½š
        if negative_score > 0:
            # ä½¿ç”¨æŒ‡æ•°è¡°å‡è¿›è¡Œæƒ©ç½š
            score *= math.exp(-negative_score)
        
        # 3. ä¼˜å…ˆçº§è°ƒæ•´
        priority = config.get("priority", 1.0)
        if priority > 1.0:
            # ä¼˜å…ˆçº§ä¹˜æ•°ï¼šé«˜ä¼˜å…ˆçº§ç±»åˆ«è·å¾—é¢å¤–åŠ æˆ
            priority_multiplier = 1.0 + (priority - 1.0) * 0.1
            score *= priority_multiplier
        
        # è®°å½•å¾—åˆ†å’ŒåŒ¹é…ä¿¡æ¯
        if score > 0:
            scores[category] = score
            match_details[category] = matches
    
    # 4. ç±»åˆ«é€‰æ‹©å’Œç½®ä¿¡åº¦è®¡ç®—
    if not scores:
        return []
    
    # è·å–æœ€é«˜å¾—åˆ†
    max_score = max(scores.values())
    
    # ç­›é€‰å€™é€‰ç±»åˆ«ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é˜ˆå€¼ï¼‰
    candidate_categories = []
    for category, score in scores.items():
        # è·å–è¯¥ç±»åˆ«çš„é˜ˆå€¼é…ç½®
        threshold_config = CATEGORY_THRESHOLDS.get(category, {})
        threshold = threshold_config.get("threshold", 1.0)
        
        # å¦‚æœå¾—åˆ†è¶…è¿‡é˜ˆå€¼ï¼Œåˆ™åŠ å…¥å€™é€‰ç±»åˆ«
        if score >= threshold:
            candidate_categories.append((category, score))
    
    # å¦‚æœæ²¡æœ‰å€™é€‰ç±»åˆ«ï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼ˆç•¥è¿‡è¯¥è®ºæ–‡ï¼‰
    if not candidate_categories:
        return []
    
    # æŒ‰å¾—åˆ†é™åºæ’åº
    candidate_categories.sort(key=lambda x: x[1], reverse=True)
    
    # 5. å­ç±»åˆ«åˆ†ç±»
    subcategory_info = None
    if candidate_categories:
        best_category = candidate_categories[0][0]
        subcategory_info = classify_subcategory(title, abstract, best_category, categories_config)
    
    # 6. æ„å»ºåˆ†ç±»è§£é‡Š
    classification_explanation = {
        "max_score": max_score,
        "total_categories": len(candidate_categories),
        "top_matches": [
            {
                "category": cat,
                "score": score,
                "details": match_details.get(cat, [])
            }
            for cat, score in candidate_categories[:3]
        ]
    }
    
    # 7. è¿”å›ç»“æœ
    result = []
    for category, score in candidate_categories:
        result.append((category, score, subcategory_info if category == candidate_categories[0][0] else None, classification_explanation if category == candidate_categories[0][0] else None))
    
    return result


def classify_subcategory(title: str, abstract: str, main_category: str, categories_config: Dict) -> Tuple[str, float]:
    """
    å¯¹è®ºæ–‡è¿›è¡Œå­ç±»åˆ«åˆ†ç±»
    
    Args:
        title: è®ºæ–‡æ ‡é¢˜
        abstract: è®ºæ–‡æ‘˜è¦
        main_category: ä¸»ç±»åˆ«
        categories_config: ç±»åˆ«é…ç½®
    
    Returns:
        Tuple[str, float]: (å­ç±»åˆ«åç§°, ç½®ä¿¡åº¦)
    """
    # è·å–ä¸»ç±»åˆ«çš„å­ç±»åˆ«é…ç½®ï¼ˆä»CATEGORY_THRESHOLDSè·å–ï¼‰
    from ai_categories_config import CATEGORY_THRESHOLDS, CATEGORY_KEYWORDS
    main_config = CATEGORY_THRESHOLDS.get(main_category, {})
    subcategories = main_config.get("subcategories", {})
    
    if not subcategories:
        return (None, 0.0)
    
    # ä½¿ç”¨å­ç±»åˆ«å…³é”®è¯è¿›è¡ŒåŒ¹é…
    title_lower = title.lower()
    abstract_lower = abstract.lower()
    
    # ä¸ºæ¯ä¸ªå­ç±»åˆ«å®šä¹‰å…³é”®è¯ï¼ˆä»CATEGORY_KEYWORDSä¸­æå–ç›¸å…³å…³é”®è¯ï¼‰
    subcategory_keywords = {
        # AI Agents å­ç±»åˆ«å…³é”®è¯
        "å•ä»£ç†è§„åˆ’ä¸å·¥å…·ä½¿ç”¨ (Single Agent Planning & Tool Use)": ["autonomous agent", "tool use", "ReAct", "reflexion", "agent planning", "tool-using", "function calling", "API calling", "tool calling", "single agent", "individual agent"],
        "å¤šä»£ç†åä½œç³»ç»Ÿ (Multi-Agent Collaboration)": ["multi-agent system", "multi-agent collaboration", "agent society", "emergent behavior", "multi-agent", "agent interaction", "agent cooperation", "agent coordination", "agent team", "cooperative agent"],
        "é•¿é“¾æ¨ç†ä¸æ€è€ƒé“¾ (Long-Chain Reasoning & CoT)": ["chain-of-thought", "o1-like", "test-time compute", "long-chain reasoning", "reasoning model", "CoT", "reasoning capability", "complex reasoning", "logical reasoning"],
        "ä¸Šä¸‹æ–‡å·¥ç¨‹ (Context Engineering)": ["context engineering", "agentic context", "dynamic context", "context optimization", "in-context management", "context management", "context compression", "RAG"],
        "Agentè¯„ä¼°ä¸åŸºå‡† (Agent Evaluation & Benchmarks)": ["agent benchmark", "agent evaluation", "GAIA", "WebArena", "agent performance", "agent testing", "agent test", "agent metric"],
        "Agentic Workflowä¸è‡ªåŠ¨åŒ– (Agentic Workflow & Automation)": ["agentic workflow", "agent orchestration", "long-horizon task", "task decomposition", "workflow automation", "autonomous execution", "agentic system", "agent workflow", "autonomous workflow"],

        # å¤šæ¨¡æ€æ¨¡å‹ å­ç±»åˆ«å…³é”®è¯
        "è§†è§‰-è¯­è¨€æ¨¡å‹ (Vision-Language Models, VLM)": ["vision-language model", "VLM", "image-text alignment", "visual question answering", "vision-language", "visual-language", "multimodal transformer", "visual-language model", "image-text"],
        "è§†é¢‘ä¸æ—¶åºå¤šæ¨¡æ€ (Video & Temporal Multimodal)": ["video understanding", "video-language", "temporal multimodal", "long video model", "video captioning", "video-text", "video generation", "action recognition", "video analysis"],
        "éŸ³é¢‘-è§†è§‰-æ–‡æœ¬èåˆ (Audio-Visual-Text Fusion)": ["audio-visual", "speech multimodal", "audio-language model", "audio-text", "speech-language", "audio event", "music generation", "multimodal audio"],
        "3D/4Dä¸ç©ºé—´å¤šæ¨¡æ€ (3D/4D & Spatial Multimodal)": ["3D multimodal", "4D generation", "gaussian splatting", "spatial understanding", "3D reconstruction", "neural rendering", "NeRF", "3D vision", "point cloud", "spatial multimodal"],
        "ç”Ÿæˆå¼å¤šæ¨¡æ€ (Generative Multimodal)": ["generative multimodal", "diffusion model multimodal", "generative AI", "AIGC", "multimodal generation", "text-to-image", "text-to-video", "image generation", "multimodal generation"],
        "ç»Ÿä¸€å¤šæ¨¡æ€é¢„è®­ç»ƒ (Unified Multimodal Pretraining)": ["unified multimodal", "any-to-any", "multimodal foundation model", "omnimodal", "multimodal pretraining", "cross-modal pretraining", "unified model", "multimodal pretraining"],

        # å¤§æ¨¡å‹é«˜æ•ˆè®­ç»ƒä¸æ¨ç† å­ç±»åˆ«å…³é”®è¯
        "æ¨¡å‹å‹ç¼©ä¸é‡åŒ– (Model Compression & Quantization)": ["model compression", "quantization", "pruning", "model quantization", "post-training quantization", "quantization-aware training", "network pruning", "model pruning", "low-rank adaptation"],
        "æ¨ç†åŠ é€ŸæŠ€æœ¯ (Inference Acceleration)": ["inference optimization", "speculative decoding", "flash attention", "KV cache", "inference acceleration", "fast inference", "inference speedup", "KV cache optimization", "inference speed"],
        "æ··åˆä¸“å®¶æ¨¡å‹ (Mixture of Experts, MoE)": ["mixture of experts", "MoE", "sparse MoE", "expert routing", "sparse expert", "mixture-of-experts"],
        "åˆæˆæ•°æ®ç”Ÿæˆ (Synthetic Data Generation)": ["synthetic data generation", "data distillation", "self-reward data", "synthetic data", "data augmentation", "self-supervised data", "synthetic training"],
        "èƒ½æ•ˆä¸å¯æŒç»­è®­ç»ƒ (Energy Efficiency & Sustainable Training)": ["energy efficient AI", "green AI", "carbon aware computing", "hardware-aware training", "low-power training", "energy optimization", "sustainable training", "carbon footprint", "energy consumption"],

        # å…·èº«æ™ºèƒ½ä¸æœºå™¨äºº å­ç±»åˆ«å…³é”®è¯
        "æœºå™¨äººå­¦ä¹ åŸºç¡€ (Robot Learning Foundations)": ["robot learning", "reinforcement learning robotics", "imitation learning robot", "robotic learning", "robotics", "robot control", "policy learning", "robot policy"],
        "ä»¿çœŸåˆ°ç°å®è¿ç§» (Sim-to-Real Transfer)": ["sim-to-real", "domain randomization", "sim2real transfer", "simulation to reality", "domain adaptation robotics", "real-world robot", "sim2real"],
        "ä¸–ç•Œæ¨¡å‹ä¸é¢„æµ‹ (World Model & Prediction)": ["world model", "video prediction robotics", "physical reasoning", "world modeling", "environment model", "physics-based", "predictive model"],
        "åŸºç¡€æ¨¡å‹åœ¨æœºå™¨äºº (Foundation Models for Robotics)": ["foundation model robotics", "large model robotics", "RT-X", "embodied foundation model", "robot foundation model", "embodied pretraining", "foundation model robot"],
        "çµå·§æ“ä½œä¸äººå½¢æœºå™¨äºº (Dexterous Manipulation & Humanoids)": ["dexterous manipulation", "humanoid robot", "bipedal locomotion", "grasping", "manipulation", "hand manipulation", "multi-finger", "fine manipulation"],

        # AI Safety, Alignment & Interpretability å­ç±»åˆ«å…³é”®è¯
        "ä»·å€¼å¯¹é½ä¸å®ªæ³•AI (Value Alignment & Constitutional AI)": ["AI alignment", "RLHF", "constitutional AI", "preference optimization", "value alignment", "RLAIF", "human feedback", "DPO", "alignment", "constitutional AI"],
        "æœºåˆ¶å¯è§£é‡Šæ€§ (Mechanistic Interpretability)": ["mechanistic interpretability", "circuit discovery", "superposition", "interpretability", "model interpretability", "explainable AI", "XAI", "feature analysis", "mechanistic interpretability"],
        "å¹»è§‰ä¸é²æ£’æ€§ (Hallucination & Robustness)": ["hallucination mitigation", "adversarial robustness", "out-of-distribution", "hallucination reduction", "factual accuracy", "factuality", "adversarial attack", "robustness", "adversarial defense"],
        "çº¢é˜Ÿæµ‹è¯•ä¸å®‰å…¨è¯„ä¼° (Red Teaming & Safety Evaluation)": ["red teaming", "jailbreak", "AI safety benchmark", "red team", "adversarial testing", "safety testing", "safety evaluation"],
        "éšç§ä¸å…¬å¹³æ€§ (Privacy & Fairness)": ["differential privacy AI", "bias mitigation", "poisoning attack", "privacy preservation", "differential privacy", "privacy-preserving", "data privacy", "fairness", "privacy preservation"],

        # Domain-Specific & Personalized AI å­ç±»åˆ«å…³é”®è¯
        "ä¸ªæ€§åŒ–å¤§æ¨¡å‹ (Personalized LLM)": ["personalized LLM", "personal AI agent", "user adaptation", "personalized language model", "user-specific", "personalization", "custom LLM"],
        "è”é‚¦ä¸éšç§ä¿æŠ¤å­¦ä¹  (Federated & Privacy-Preserving Learning)": ["federated learning", "federated", "distributed learning", "privacy-preserving learning", "federated learning"],
        "AI for Science": ["AI for science", "scientific discovery", "scientific AI", "AI research", "AI for science"],
        "åŒ»ç–—å¥åº·AI (Medical & Healthcare AI)": ["medical AI", "healthcare AI", "clinical AI", "medical NLP", "health NLP", "medical AI", "healthcare AI"],
        "é‡‘èä¸æ³•å¾‹AI (Financial & Legal AI)": ["financial AI", "fintech AI", "financial NLP", "legal AI", "legal NLP", "legal tech", "financial AI", "fintech AI"],
    }
    
    # è®¡ç®—æ¯ä¸ªå­ç±»åˆ«çš„å¾—åˆ†
    subcategory_scores = defaultdict(float)
    for subcategory, keywords in subcategory_keywords.items():
        # åªæ£€æŸ¥å±äºå½“å‰ä¸»ç±»åˆ«çš„å­ç±»åˆ«
        if subcategory not in subcategories:
            continue
        
        score = 0.0
        for keyword in keywords:
            keyword_lower = keyword.lower()
            # æ ‡é¢˜åŒ¹é…æƒé‡æ›´é«˜
            if keyword_lower in title_lower:
                score += 2.0
            # æ‘˜è¦åŒ¹é…
            elif keyword_lower in abstract_lower:
                score += 1.0
        
        # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•å•è¯çº§åˆ«çš„åŒ¹é…
        if score == 0:
            # å°†å…³é”®è¯æ‹†åˆ†ä¸ºå•è¯
            for keyword in keywords:
                keyword_words = keyword.lower().split()
                # å¦‚æœå…³é”®è¯æ˜¯å¤šä¸ªå•è¯ï¼Œæ£€æŸ¥æ˜¯å¦å¤§éƒ¨åˆ†å•è¯å‡ºç°åœ¨æ ‡é¢˜æˆ–æ‘˜è¦ä¸­
                if len(keyword_words) > 1:
                    title_word_count = sum(1 for word in keyword_words if word in title_lower)
                    abstract_word_count = sum(1 for word in keyword_words if word in abstract_lower)
                    
                    # å¦‚æœå¤§éƒ¨åˆ†å…³é”®è¯å•è¯å‡ºç°åœ¨æ ‡é¢˜ä¸­ï¼Œç»™äºˆè¾ƒé«˜åˆ†æ•°ï¼ˆé™ä½é˜ˆå€¼ä»0.7åˆ°0.5ï¼‰
                    if title_word_count >= len(keyword_words) * 0.5:
                        score += 1.5 * (title_word_count / len(keyword_words))
                    # å¦‚æœå¤§éƒ¨åˆ†å…³é”®è¯å•è¯å‡ºç°åœ¨æ‘˜è¦ä¸­ï¼Œç»™äºˆä¸­ç­‰åˆ†æ•°ï¼ˆé™ä½é˜ˆå€¼ä»0.7åˆ°0.5ï¼‰
                    elif abstract_word_count >= len(keyword_words) * 0.5:
                        score += 0.8 * (abstract_word_count / len(keyword_words))
                else:
                    # å•ä¸ªå•è¯ï¼Œæ£€æŸ¥æ˜¯å¦åœ¨æ ‡é¢˜æˆ–æ‘˜è¦ä¸­
                    if keyword_words[0] in title_lower:
                        score += 1.0
                    elif keyword_words[0] in abstract_lower:
                        score += 0.5
        
        if score > 0:
            subcategory_scores[subcategory] = score
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°å­ç±»åˆ«ï¼Œè¿”å›None
    if not subcategory_scores:
        return (None, 0.0)
    
    # è¿”å›å¾—åˆ†æœ€é«˜çš„å­ç±»åˆ«
    best_subcategory = max(subcategory_scores.items(), key=lambda x: x[1])
    return best_subcategory


def process_paper(paper, helper, categories_config):
    """å¤„ç†å•ç¯‡è®ºæ–‡"""
    try:
        # æå–è®ºæ–‡ä¿¡æ¯
        title = paper.title
        authors = ', '.join([author.name for author in paper.authors[:8]])
        abstract = paper.summary
        pdf_url = paper.pdf_url
        published_date = paper.published
        
        # åˆ¤æ–­æ˜¯å¦æ˜¯æ›´æ–°çš„è®ºæ–‡
        is_updated = paper.updated > published_date
        
        # æå–ä»£ç é“¾æ¥
        code_url = extract_github_link(paper)
        if not code_url:
            code_url = 'None'
        
        # ç¿»è¯‘æ ‡é¢˜
        title_zh = helper.translate_title(title, abstract)
        
        # åˆ†æè®ºæ–‡æ ¸å¿ƒè´¡çŒ®
        contribution_info = helper.analyze_paper_contribution(title, abstract)
        
        # ä½¿ç”¨LLMå¯¹è®ºæ–‡è¿›è¡Œåˆ†ç±»
        category_results = helper.classify_paper_with_llm(title, abstract)
        if not category_results:
            # æ— æ³•åˆ†ç±»åˆ°é¢„å®šä¹‰ç±»åˆ«ï¼Œç•¥è¿‡è¯¥è®ºæ–‡
            return None
        
        category = category_results[0][0]
        confidence = category_results[0][1]
        subcategory_info = category_results[0][2]
        
        # æå–å­ç±»åˆ«
        if subcategory_info:
            subcategory = subcategory_info[0]
        else:
            # å¦‚æœæ²¡æœ‰å­ç±»åˆ«ï¼Œä»ç„¶ä¿ç•™è®ºæ–‡ï¼ˆåªè¦æœ‰ä¸»ç±»åˆ«ï¼‰
            subcategory = ""
        
        # æ„å»ºè®ºæ–‡å­—å…¸
        paper_dict = {
            'title': title,
            'title_zh': title_zh,
            'authors': authors,
            'abstract': abstract,
            'pdf_url': pdf_url,
            'github_url': code_url,
            'published_date': published_date,
            'is_updated': is_updated,
            'category': category,
            'subcategory': subcategory,
            'confidence': confidence,
        }
        
        # æ·»åŠ æ ¸å¿ƒè´¡çŒ®ä¿¡æ¯
        if contribution_info and "æ ¸å¿ƒè´¡çŒ®" in contribution_info:
            paper_dict["æ ¸å¿ƒè´¡çŒ®"] = contribution_info["æ ¸å¿ƒè´¡çŒ®"]
        
        return paper_dict
        
    except Exception as e:
        print(f"å¤„ç†è®ºæ–‡æ—¶å‡ºé”™: {str(e)}")
        traceback.print_exc()
        return None


def get_ai_papers():
    """è·å–AI/ç®—æ³•è®ºæ–‡çš„ä¸»å‡½æ•°"""
    print("=" * 80)
    print("AI/ç®—æ³•è®ºæ–‡æ¯æ—¥æ›´æ–°ç³»ç»Ÿ")
    print("=" * 80)
    
    # åˆå§‹åŒ–ChatGLMåŠ©æ‰‹
    print("\nğŸ¤– åˆå§‹åŒ–AIåŠ©æ‰‹...")
    helper = ChatGLMHelper()
    
    # å¯¼å…¥å…³é”®è¯é…ç½®
    from ai_categories_config import CATEGORY_KEYWORDS
    
    # åˆ›å»ºArXivå®¢æˆ·ç«¯
    print("\nğŸ“¡ è¿æ¥ArXiv...")
    client = arxiv.Client(
        page_size=100,  # æ¯é¡µè·å–100ç¯‡è®ºæ–‡
        delay_seconds=5,  # è¯·æ±‚é—´éš”10ç§’
        num_retries=5    # å¤±è´¥é‡è¯•5æ¬¡
    )

    # è®¡ç®—ç›®æ ‡æ—¥æœŸèŒƒå›´ï¼ˆç”¨äºè¿‡æ»¤ï¼‰
    target_date = datetime.now() - timedelta(days=QUERY_DAYS_AGO)
    target_date_str = target_date.strftime('%Y-%m-%d')
    
    # æ„å»ºæŸ¥è¯¢ - ä½¿ç”¨å¤šä¸ªç±»åˆ«ï¼ˆä¸åœ¨æŸ¥è¯¢ä¸­è¿‡æ»¤æ—¥æœŸï¼Œåœ¨ä»£ç ä¸­è¿‡æ»¤ï¼‰
    query = ' OR '.join([f'cat:{cat}' for cat in ARXIV_CATEGORIES])
    
    search = arxiv.Search(
        query=query,
        max_results=MAX_RESULTS,
        sort_by=arxiv.SortCriterion.SubmittedDate,
        sort_order=arxiv.SortOrder.Descending  # ç¡®ä¿æŒ‰æ—¶é—´é™åºæ’åº
    )

    # åˆ›å»ºçº¿ç¨‹æ± 
    total_papers = 0
    classified_papers = 0  # æˆåŠŸåˆ†ç±»çš„è®ºæ–‡æ•°
    papers_by_category = defaultdict(list)

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†è®ºæ–‡
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # åˆ›å»ºè¿›åº¦æ¡
        print("\nğŸ” å¼€å§‹è·å–è®ºæ–‡...")
        results = client.results(search)
        
        # åˆ›å»ºæ€»è¿›åº¦æ¡
        total_pbar = tqdm(
            total=MAX_RESULTS,
            desc="æ€»è¿›åº¦",
            unit="ç¯‡",
            position=0,
            leave=True
        )
        
        # åˆ›å»ºæ‰¹å¤„ç†è¿›åº¦æ¡
        batch_pbar = tqdm(
            total=0,  # åˆå§‹å€¼ä¸º0ï¼Œåé¢ä¼šæ›´æ–°
            desc="å½“å‰æ‰¹æ¬¡",
            unit="ç¯‡",
            position=1,
            leave=True
        )
        
        # æ‰¹é‡å¤„ç†è®ºæ–‡
        batch_size = 10  # æ¯æ‰¹å¤„ç†10ç¯‡è®ºæ–‡
        papers = []
        futures = []
        
        for result in results:
            if total_papers >= MAX_RESULTS:
                break
            
            # è¿‡æ»¤ï¼šåªä¿ç•™ç›®æ ‡æ—¥æœŸçš„è®ºæ–‡
            paper_date = result.published.strftime('%Y-%m-%d')
            if paper_date != target_date_str:
                continue
            
            papers.append(result)
            total_papers += 1
            total_pbar.update(1)
            
            # å½“è¾¾åˆ°æ‰¹é‡å¤§å°æ—¶ï¼Œæäº¤å¤„ç†ä»»åŠ¡
            if len(papers) >= batch_size:
                batch_pbar.total = len(papers)
                batch_pbar.reset()
                
                # æäº¤å½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰è®ºæ–‡å¤„ç†ä»»åŠ¡
                for paper in papers:
                    future = executor.submit(process_paper, paper, helper, CATEGORY_KEYWORDS)
                    futures.append(future)
                
                # ç­‰å¾…å½“å‰æ‰¹æ¬¡å®Œæˆ
                for future in as_completed(futures):
                    try:
                        paper_dict = future.result()
                        if paper_dict:
                            category = paper_dict['category']
                            papers_by_category[category].append(paper_dict)
                            classified_papers += 1  # æˆåŠŸåˆ†ç±»çš„è®ºæ–‡æ•°+1
                    except Exception as e:
                        print(f"å¤„ç†è®ºæ–‡æ—¶å‡ºé”™: {str(e)}")
                        traceback.print_exc()
                
                # æ¸…ç©ºæ‰¹æ¬¡
                papers = []
                futures = []
                batch_pbar.update(batch_size)
        
        # å¤„ç†å‰©ä½™çš„è®ºæ–‡
        if papers:
            batch_pbar.total = len(papers)
            batch_pbar.reset()
            
            for paper in papers:
                future = executor.submit(process_paper, paper, helper, CATEGORY_KEYWORDS)
                futures.append(future)
            
            for future in as_completed(futures):
                try:
                    paper_dict = future.result()
                    if paper_dict:
                        category = paper_dict['category']
                        papers_by_category[category].append(paper_dict)
                        classified_papers += 1  # æˆåŠŸåˆ†ç±»çš„è®ºæ–‡æ•°+1
                except Exception as e:
                    print(f"å¤„ç†è®ºæ–‡æ—¶å‡ºé”™: {str(e)}")
                    traceback.print_exc()
            
            batch_pbar.update(len(papers))
        
        # å…³é—­è¿›åº¦æ¡
        total_pbar.close()
        batch_pbar.close()
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    # é‡æ–°ç»Ÿè®¡å®é™…ä¿å­˜çš„è®ºæ–‡æ•°ï¼ˆæœ‰ä¸»ç±»åˆ«çš„è®ºæ–‡ï¼‰
    actual_saved_papers = 0
    for category, papers in papers_by_category.items():
        for paper in papers:
            if paper.get('category'):
                actual_saved_papers += 1
    
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"è·å–è®ºæ–‡æ€»æ•°: {total_papers} ç¯‡")
    print(f"å®é™…ä¿å­˜è®ºæ–‡æ•°: {actual_saved_papers} ç¯‡")
    print(f"æœªåˆ†ç±»è®ºæ–‡æ•°: {total_papers - actual_saved_papers} ç¯‡")
    for category, papers in papers_by_category.items():
        if papers:
            print(f"  {category}: {len(papers)} ç¯‡")
    
    # ç”ŸæˆMarkdownæ–‡ä»¶
    print("\nğŸ“ ç”ŸæˆMarkdownæ–‡ä»¶...")
    
    # è®¡ç®—ç›®æ ‡æ—¥æœŸ
    target_date = datetime.now() - timedelta(days=QUERY_DAYS_AGO)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(script_dir, '..', 'data')
    local_dir = os.path.join(script_dir, '..', 'local')
    
    # åˆ›å»ºå¹´-æœˆç›®å½•
    year_month = target_date.strftime('%Y-%m')
    data_year_month = os.path.join(data_dir, year_month)
    local_year_month = os.path.join(local_dir, year_month)
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(data_year_month, exist_ok=True)
    os.makedirs(local_year_month, exist_ok=True)
    
    # ç”Ÿæˆæ–‡ä»¶å
    filename = f"{target_date.strftime('%Y-%m-%d')}.md"
    table_filepath = os.path.join(data_year_month, filename)
    detailed_filepath = os.path.join(local_year_month, filename)

    # ç”Ÿæˆæ ‡é¢˜
    title = f"## [UPDATED!] **{target_date.strftime('%Y-%m-%d')}** (Update Time)\n\n"

    # ä¿å­˜è¡¨æ ¼æ ¼å¼çš„markdownæ–‡ä»¶åˆ°data/å¹´-æœˆç›®å½•
    with open(table_filepath, 'w', encoding='utf-8') as f:
        f.write(title)
        f.write(df_to_markdown_table(papers_by_category, target_date))

    # ä¿å­˜è¯¦ç»†æ ¼å¼çš„markdownæ–‡ä»¶åˆ°local/å¹´-æœˆç›®å½•
    with open(detailed_filepath, 'w', encoding='utf-8') as f:
        f.write(title)
        f.write(df_to_markdown_detailed(papers_by_category, target_date))

    print(f"\nè¡¨æ ¼æ ¼å¼æ–‡ä»¶å·²ä¿å­˜åˆ°: {table_filepath}")
    print(f"è¯¦ç»†æ ¼å¼æ–‡ä»¶å·²ä¿å­˜åˆ°: {detailed_filepath}")


if __name__ == "__main__":
    # ç›´æ¥è¿è¡ŒæŸ¥è¯¢
    get_ai_papers()
